{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f477e3cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T02:45:21.937694Z",
     "start_time": "2023-02-27T02:45:19.063557Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode, split, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee93c8a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T02:45:33.998120Z",
     "start_time": "2023-02-27T02:45:24.284177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:45:26 WARN Utils: Your hostname, Kaustubhs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.21 instead (on interface en0)\n",
      "23/02/26 18:45:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/26 18:45:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"TwitterStreaming\").getOrCreate()\n",
    "\n",
    "# create a StreamingContext with batch interval of 1 second\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# create a DStream that reads data from port 5554\n",
    "lines = ssc.socketTextStream(\"localhost\", 5554)\n",
    "\n",
    "def preprocessing(lines):\n",
    "    words = lines.map(lambda x: x.strip()) \\\n",
    "                 .filter(lambda x: x != '') \\\n",
    "                 .toDF(schema=StringType()) \\\n",
    "                 .select(explode(split('value', '\\s+')).alias('word')) \\\n",
    "                 .na.replace('', None) \\\n",
    "                 .na.drop() \\\n",
    "                 .withColumn('word', regexp_replace('word', r'http\\S+', '')) \\\n",
    "                 .withColumn('word', regexp_replace('word', '@\\w+', '')) \\\n",
    "                 .withColumn('word', regexp_replace('word', '#', '')) \\\n",
    "                 .withColumn('word', regexp_replace('word', 'RT', '')) \\\n",
    "                 .withColumn('word', regexp_replace('word', ':', ''))\n",
    "    return words\n",
    "\n",
    "# preprocess non-empty RDDs and store clean tweets in a Spark DataFrame\n",
    "clean_tweets = lines.foreachRDD(lambda rdd: process_rdd(rdd) if not rdd.isEmpty() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53fa7b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T02:45:48.303983Z",
     "start_time": "2023-02-27T02:45:48.084159Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:45:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:53 WARN BlockManager: Block input-0-1677465953600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:54 WARN BlockManager: Block input-0-1677465953800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:54 WARN BlockManager: Block input-0-1677465954200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:54 WARN BlockManager: Block input-0-1677465954600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:55 WARN BlockManager: Block input-0-1677465955600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:56 WARN BlockManager: Block input-0-1677465956600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:57 WARN BlockManager: Block input-0-1677465957600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:58 WARN BlockManager: Block input-0-1677465957800 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "# start the streaming context\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e1476c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T02:46:00.355192Z",
     "start_time": "2023-02-27T02:45:58.925383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:45:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:58 WARN BlockManager: Block input-0-1677465958600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:59 WARN BlockManager: Block input-0-1677465958800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:45:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:45:59 WARN BlockManager: Block input-0-1677465959600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:00 WARN BlockManager: Block input-0-1677465959800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:00 ERROR JobScheduler: Error running job streaming job 1677465954000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:00 ERROR JobScheduler: Error running job streaming job 1677465955000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:00 WARN BlockManager: Block input-0-1677465960600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:00 ERROR JobScheduler: Error running job streaming job 1677465956000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:01 ERROR JobScheduler: Error running job streaming job 1677465957000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:01 ERROR JobScheduler: Error running job streaming job 1677465958000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:01 ERROR JobScheduler: Error running job streaming job 1677465959000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:01 WARN BlockManager: Block input-0-1677465961600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:01 ERROR JobScheduler: Error running job streaming job 1677465960000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:02 ERROR JobScheduler: Error running job streaming job 1677465961000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:02 ERROR JobScheduler: Error running job streaming job 1677465962000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:02 WARN BlockManager: Block input-0-1677465962600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:03 ERROR JobScheduler: Error running job streaming job 1677465963000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:03 WARN BlockManager: Block input-0-1677465963600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:04 WARN BlockManager: Block input-0-1677465963800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:04 ERROR JobScheduler: Error running job streaming job 1677465964000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:04 WARN BlockManager: Block input-0-1677465964600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:05 ERROR JobScheduler: Error running job streaming job 1677465965000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:05 WARN BlockManager: Block input-0-1677465965600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:06 WARN BlockManager: Block input-0-1677465965800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:06 ERROR JobScheduler: Error running job streaming job 1677465966000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:06 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:06 WARN BlockManager: Block input-0-1677465966600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:07 ERROR JobScheduler: Error running job streaming job 1677465967000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:07 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:07 WARN BlockManager: Block input-0-1677465967600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "23/02/26 18:46:08 ERROR JobScheduler: Error running job streaming job 1677465968000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1990015b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T02:46:12.987539Z",
     "start_time": "2023-02-27T02:46:08.348645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:08 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:08 WARN BlockManager: Block input-0-1677465968600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:09 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:09 WARN BlockManager: Block input-0-1677465968800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:09 ERROR JobScheduler: Error running job streaming job 1677465969000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:09 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:09 WARN BlockManager: Block input-0-1677465969600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:10 WARN BlockManager: Block input-0-1677465969800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:10 ERROR JobScheduler: Error running job streaming job 1677465970000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:10 WARN BlockManager: Block input-0-1677465970600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:11 ERROR JobScheduler: Error running job streaming job 1677465971000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:11 WARN BlockManager: Block input-0-1677465971600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:12 WARN BlockManager: Block input-0-1677465971800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:12 WARN BlockManager: Block input-0-1677465972000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:12 WARN BlockManager: Block input-0-1677465972600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\nmismatched input '\"SELECT word FROM tweets\"' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\n\n== SQL ==\n\"SELECT word FROM tweets\"\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m new_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m      4\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep( \u001b[38;5;241m3\u001b[39m )\n\u001b[0;32m----> 5\u001b[0m     top_10_tweets \u001b[38;5;241m=\u001b[39m \u001b[43msqlContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT word FROM tweets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     top_10_df \u001b[38;5;241m=\u001b[39m top_10_tweets\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      7\u001b[0m     new_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pyspark/sql/context.py:437\u001b[0m, in \u001b[0;36mSQLContext.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \nmismatched input '\"SELECT word FROM tweets\"' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\n\n== SQL ==\n\"SELECT word FROM tweets\"\n^^^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:13 WARN BlockManager: Block input-0-1677465972800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:13 ERROR JobScheduler: Error running job streaming job 1677465972000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:13 ERROR JobScheduler: Error running job streaming job 1677465973000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:13 WARN BlockManager: Block input-0-1677465973600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:14 WARN BlockManager: Block input-0-1677465973800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:14 WARN BlockManager: Block input-0-1677465974000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:14 ERROR JobScheduler: Error running job streaming job 1677465974000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:14 WARN BlockManager: Block input-0-1677465974600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:15 ERROR JobScheduler: Error running job streaming job 1677465975000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:15 WARN BlockManager: Block input-0-1677465975200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:15 WARN BlockManager: Block input-0-1677465975600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:16 ERROR JobScheduler: Error running job streaming job 1677465976000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:16 WARN BlockManager: Block input-0-1677465976600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:17 ERROR JobScheduler: Error running job streaming job 1677465977000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:18 WARN BlockManager: Block input-0-1677465978600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:19 WARN BlockManager: Block input-0-1677465979000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:19 ERROR JobScheduler: Error running job streaming job 1677465979000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:19 WARN BlockManager: Block input-0-1677465979600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:20 WARN BlockManager: Block input-0-1677465979800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:20 ERROR JobScheduler: Error running job streaming job 1677465980000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:20 WARN BlockManager: Block input-0-1677465980600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:21 ERROR JobScheduler: Error running job streaming job 1677465981000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:21 WARN BlockManager: Block input-0-1677465981600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:22 ERROR JobScheduler: Error running job streaming job 1677465982000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:22 WARN BlockManager: Block input-0-1677465982600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:23 ERROR JobScheduler: Error running job streaming job 1677465983000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:23 WARN BlockManager: Block input-0-1677465983600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:24 ERROR JobScheduler: Error running job streaming job 1677465984000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:24 WARN BlockManager: Block input-0-1677465984600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:25 ERROR JobScheduler: Error running job streaming job 1677465985000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:25 WARN BlockManager: Block input-0-1677465985600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:26 WARN BlockManager: Block input-0-1677465985800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:26 ERROR JobScheduler: Error running job streaming job 1677465986000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:26 WARN BlockManager: Block input-0-1677465986600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:27 WARN BlockManager: Block input-0-1677465986800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:27 ERROR JobScheduler: Error running job streaming job 1677465987000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:27 WARN BlockManager: Block input-0-1677465987600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:28 ERROR JobScheduler: Error running job streaming job 1677465988000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:28 WARN BlockManager: Block input-0-1677465988600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:29 ERROR JobScheduler: Error running job streaming job 1677465989000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:29 WARN BlockManager: Block input-0-1677465989600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:30 ERROR JobScheduler: Error running job streaming job 1677465990000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:30 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:30 WARN BlockManager: Block input-0-1677465990600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:31 ERROR JobScheduler: Error running job streaming job 1677465991000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/02/26 18:46:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:31 WARN BlockManager: Block input-0-1677465991200 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:31 WARN BlockManager: Block input-0-1677465991600 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/02/26 18:46:32 WARN BlockManager: Block input-0-1677465991800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/02/26 18:46:32 ERROR JobScheduler: Error running job streaming job 1677465992000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/var/folders/fc/dcskpc694gg68q4bvnmpm3mc0000gn/T/ipykernel_59394/72639139.py\", line 27, in <lambda>\n",
      "    spark.createDataFrame(preprocessing(rdd)) if not rdd.isEmpty() else None)\n",
      "  File \"/Users/Kaustubh/miniconda3/lib/python3.8/site-packages/pyspark/sql/session.py\", line 658, in createDataFrame\n",
      "    raise TypeError(\"data is already a DataFrame\")\n",
      "TypeError: data is already a DataFrame\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "new_count = 0\n",
    "while new_count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( '\"SELECT word FROM tweets\"' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    new_count += 1\n",
    "print(top_10_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f4afc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T02:46:33.056631Z",
     "start_time": "2023-02-27T02:46:32.502659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/26 18:46:32 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/02/26 18:46:32 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "23/02/26 18:46:32 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "23/02/26 18:46:32 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\t... 2 more\n"
     ]
    }
   ],
   "source": [
    "# stop the SparkSession\n",
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
